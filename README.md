# ADLS to Azure SQL Data Pipeline using Apache Airflow

This project automates the movement of analytical data from **Azure Data Lake Storage (ADLS)** to **Azure SQL Database** using **Apache Airflow**. It demonstrates a real-world ETL workflow where data is extracted, transformed, and loaded for analysis and reporting.

The dataset used is a **Swiggy Analysis Report**, which contains restaurant and customer insights. The project includes two main parts:
- **Swiggy Data Analysis** â€“ Data cleaning, transformation, and visualization using Python and SQL.
- **Airflow Data Pipeline** â€“ A DAG that automates data transfer from ADLS to Azure SQL using `WasbHook` and `MsSqlHook`.

### ðŸ§  Tech Stack
- **Programming:** Python, SQL  
- **Orchestration:** Apache Airflow  
- **Cloud:** Microsoft Azure  
- **Storage:** Azure Data Lake Storage  
- **Database:** Azure SQL Database  
- **Deployment:** Docker, Azure VM  

### âœ… Key Outcomes
- Automated ETL pipeline for cloud-based data integration  
- Successful orchestration of data movement using Airflow DAGs  
- Improved data accessibility and analytics readiness on Azure SQL  

### ðŸš€ Future Scope
- Add transformation logic and data validation  
- Integrate visualization tools like Power BI  
- Implement alerting and monitoring in Airflow  

**Author:** Sudhir Subramanian  
*Data Engineering Enthusiast | Python | SQL | Azure | Airflow*  
